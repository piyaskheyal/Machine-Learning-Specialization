{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5b4ef2-0243-4b6a-9691-12f8d5e98635",
   "metadata": {},
   "source": [
    "# Gradient Descent for Logistic Regression (Explicit $w$ and $b$)\n",
    "\n",
    "## Overview\n",
    "Gradient descent optimizes the parameters **weight vector $\\vec{w}$** and **bias term $b$** to minimize the log loss cost function. Here's the revised formulation with explicit $w$ and $b$:\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. **Hypothesis Function**\n",
    "\n",
    "$$\n",
    "h_{\\vec{w}, b}(\\vec{x}) = \\sigma(\\vec{w}^T \\vec{x} + b) = \\frac{1}{1 + e^{-(\\vec{w}^T \\vec{x} + b)}}\n",
    "$$\n",
    "- $\\vec{w} = [w_1, w_2, \\dots, w_n]^T$: **Weight vector** (excluding bias).  \n",
    "- $b$: **Bias term** (scalar).  \n",
    "- $\\vec{x} = [x_1, x_2, \\dots, x_n]^T$: Feature vector (**no** added $1$ for bias).  \n",
    "\n",
    "### 2. **Cost Function (Log Loss)**\n",
    "\n",
    "$$\n",
    "J(\\vec{w}, b) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(h_{\\vec{w}, b}(\\vec{x}^{(i)})) + (1-y^{(i)}) \\log(1 - h_{\\vec{w}, b}(\\vec{x}^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "### 3. **Gradients**\n",
    "#### Partial Derivative w.r.t. Weight $w_j$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\vec{w}, b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "#### Partial Derivative w.r.t. Bias $b$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\vec{w}, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent Steps\n",
    "\n",
    "### 1. **Initialize Parameters**\n",
    "- Set $\\vec{w}$ to initial values (e.g., zeros).  \n",
    "- Set $b = 0$.  \n",
    "\n",
    "### 2. **Update Rules**\n",
    "For each weight $w_j$:  \n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J(\\vec{w}, b)}{\\partial w_j}\n",
    "$$\n",
    "For bias $b$:  \n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J(\\vec{w}, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "### 3. **Repeat**\n",
    "- Update all $w_j$ and $b$ **simultaneously**.  \n",
    "- Iterate until convergence.  \n",
    "\n",
    "---\n",
    "\n",
    "## Matrix Form (Efficient Computation)\n",
    "Let:  \n",
    "- $\\mathbf{X}$: Design matrix of shape $(m \\times n)$ (**no** added bias column).  \n",
    "- $\\vec{y}$: Label vector of shape $(m \\times 1)$.  \n",
    "- $\\vec{h} = \\sigma(\\mathbf{X} \\vec{w} + b)$: Vector of predicted probabilities.  \n",
    "\n",
    "### Gradients\n",
    "- **Weight gradient vector**:  \n",
    "\n",
    "$$\n",
    "\\nabla_{\\vec{w}} J = \\frac{1}{m} \\mathbf{X}^T (\\vec{h} - \\vec{y})\n",
    "$$\n",
    "- **Bias gradient**:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (h^{(i)} - y^{(i)})\n",
    "$$\n",
    "\n",
    "### Parameter Updates\n",
    "\n",
    "$$\n",
    "\\vec{w} := \\vec{w} - \\alpha \\nabla_{\\vec{w}} J\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Algorithm\n",
    "1. Compute $\\vec{h} = \\sigma(\\mathbf{X} \\vec{w} + b)$.  \n",
    "2. Calculate gradients:  \n",
    "   - $\\nabla_{\\vec{w}} J = \\frac{1}{m} \\mathbf{X}^T (\\vec{h} - \\vec{y})$  \n",
    "   - $\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum (\\vec{h} - \\vec{y})$  \n",
    "3. Update parameters:  \n",
    "\n",
    "$$\n",
    "\\vec{w} := \\vec{w} - \\alpha \\nabla_{\\vec{w}} J\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "4. Repeat until convergence.  \n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "- **Interpretation**: The bias $b$ acts as an \"offset\" in the linear combination $\\vec{w}^T \\vec{x} + b$.  \n",
    "- **Implementation**: In code, often combine $\\vec{w}$ and $b$ into a single parameter vector $\\vec{\\theta} = [b, w_1, \\dots, w_n]^T$ by adding a column of $1$s to $\\mathbf{X}$.  \n",
    "- **Equivalence**: This formulation is mathematically identical to the $\\vec{\\theta}$ notation but separates $w$ and $b$ explicitly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72ef2e-2c22-4086-8579-3d2685db886b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
