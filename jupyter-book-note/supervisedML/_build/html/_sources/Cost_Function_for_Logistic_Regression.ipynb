{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20acbfbb-ba80-45d9-bcf1-96975cc8169b",
   "metadata": {},
   "source": [
    "# Cost Function for Logistic Regression\n",
    "\n",
    "Logistic Regression is a classification algorithm used to predict binary outcomes (0 or 1). Its cost function is derived using **maximum likelihood estimation (MLE)** and penalizes incorrect predictions probabilistically. Below is a detailed breakdown of the logistic loss function and the overall cost function.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Hypothesis Function (Logistic/Sigmoid Function)**\n",
    "The logistic regression hypothesis maps input features to a probability between 0 and 1 using the **sigmoid function**:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
    "$$\n",
    "- $\\theta$: Model parameters (weights).\n",
    "- $x$: Input feature vector.\n",
    "- $\\sigma(z)$: Sigmoid function, which squashes $z$ into $[0, 1]$.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Logistic Loss Function**\n",
    "The loss function for a single training example $(x^{(i)}, y^{(i)})$ is the **negative log-likelihood**, also known as the **logistic loss** or **cross-entropy loss**:\n",
    "\n",
    "$$\n",
    "L(h_\\theta(x^{(i)}), y^{(i)}) = \n",
    "\\begin{cases} \n",
    "-\\log(h_\\theta(x^{(i)})) & \\text{if } y^{(i)} = 1, \\\\\n",
    "-\\log(1 - h_\\theta(x^{(i)})) & \\text{if } y^{(i)} = 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "This can be compactly written as:\n",
    "\n",
    "$$\n",
    "L(h_\\theta(x^{(i)}), y^{(i)}) = -y^{(i)} \\log(h_\\theta(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Cost Function**\n",
    "The overall cost function $J(\\theta)$ is the **average loss** over all $m$ training examples:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log(h_\\theta(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
    "$$\n",
    "To prevent overfitting, a **regularization term** (e.g., L2 regularization) is often added:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log(h_\\theta(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2\n",
    "$$\n",
    "- $\\lambda$: Regularization parameter.\n",
    "- $\\theta_j$: Model weights (excluding the bias term $\\theta_0$).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Gradient of the Cost Function**\n",
    "To optimize $\\theta$, we compute the gradient of $J(\\theta)$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)} + \\frac{\\lambda}{m} \\theta_j \\quad \\text{(for } j \\geq 1 \\text{)}\n",
    "$$\n",
    "For the unregularized case ($\\lambda = 0$):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Vectorized Form**\n",
    "Let $X \\in \\mathbb{R}^{m \\times n}$ be the design matrix, $y \\in \\mathbb{R}^m$ the labels, and $\\theta \\in \\mathbb{R}^n$ the parameters:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\left[ y^T \\log(h_\\theta(X)) + (1 - y)^T \\log(1 - h_\\theta(X)) \\right] + \\frac{\\lambda}{2m} \\theta^T \\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{1}{m} X^T (h_\\theta(X) - y) + \\frac{\\lambda}{m} \\theta\n",
    "$$\n",
    "where $h_\\theta(X) = \\sigma(X\\theta)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Key Properties**\n",
    "1. **Convexity**: The logistic loss is $convex$, ensuring gradient descent converges to the global minimum.\n",
    "2. **Probabilistic Interpretation**: Minimizing $J(\\theta)$ maximizes the likelihood of the observed data.\n",
    "3. **Regularization**: The $\\lambda$-term penalizes large weights to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Why Not Mean Squared Error (MSE)?**\n",
    "MSE is unsuitable for logistic regression because:\n",
    "- The loss landscape becomes $non\\text{-}convex$.\n",
    "- Predictions are probabilities, not continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52073be9-292d-4b97-9d98-573b0a32faee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
